{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO_LunarLander_Continuous.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMr/9V8OIOGygWvbKrd4ABX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiSEw_HXXHKg","executionInfo":{"status":"ok","timestamp":1655774720034,"user_tz":-480,"elapsed":34532,"user":{"displayName":"Lab Tei","userId":"14661680927443254315"}},"outputId":"b0fb8f03-de67-4968-c383-48aedee4602f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,271 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n","Fetched 784 kB in 1s (699 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 155639 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 3.8 MB/s \n","\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.9)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.4.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.1)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 8.5 MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"]}],"source":["# Rendering problems in Colab \n","# https://stackoverflow.com/questions/63250935/nameerror-name-base-is-not-defined-while-running-open-ai-gym-in-google-colab\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","\n","!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","#=========================================================#\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","# why unwrapped\n","# https://stackoverflow.com/questions/53836136/why-unwrap-an-openai-gym\n","env = gym.make('CartPole-v0').unwrapped\n","\n","# set up matplotlib\n","is_ipython = 'inline' in matplotlib.get_backend()\n","if is_ipython:\n","    from IPython import display\n","\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["class RolloutMemory():\n","  def __init__(self, batch_size=32):\n","    self.batch_size = batch_size\n","    self.states = []\n","    self.actions = []\n","    self.rewards = []\n","    self.log_probs = []\n","    self.dones = []\n","  \n","  def clear_memory(self):\n","    del self.states[:] # delete the variables instead of assign to new empty list\n","    del self.actions[:] # it's said that it's do with machine memory, (may faster?)\n","    del self.rewards[:]\n","    del self.log_probs[:]\n","    del self.dones[:]\n","  \n","  def generate_batches(self):\n","    batch_size = self.batch_size\n","    data_length = len(self.states)\n","    batches_start_indices = np.arange(0, data_length, batch_size) # handling different length T\n","    indices = np.arange(data_length, dtype=np.int64)\n","    np.random.shuffle(indices)\n","    batches_indices = [indices[i : i + batch_size] for i in batches_start_indices]\n","\n","    return batches_indices"],"metadata":{"id":"SZ0mqwgnYlyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.distributions import MultivariateNormal\n","\n","def init_orthogonal_weights(m):\n","  if isinstance(m, nn.Linear):\n","    orthogonal_init(m.weight)\n","    nn.init.constant_(m.bias, 0.1)\n","\n","def orthogonal_init(tensor, gain=1):\n","  '''\n","  https://github.com/implementation-matters/code-for-paper/blob/094994f2bfd154d565c34f5d24a7ade00e0c5bdb/src/policy_gradients/torch_utils.py#L494\n","  Fills the input `Tensor` using the orthogonal initialization scheme from OpenAI\n","  Args:\n","      tensor: an n-dimensional `torch.Tensor`, where :math:`n \\geq 2`\n","      gain: optional scaling factor\n","  Examples:\n","      >>> w = torch.empty(3, 5)\n","      >>> orthogonal_init(w)\n","  '''\n","  if tensor.ndimension() < 2:\n","    raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n","\n","  rows = tensor.size(0)\n","  cols = tensor[0].numel()\n","  flattened = tensor.new(rows, cols).normal_(0, 1)\n","\n","  if rows < cols:\n","    flattened.t_()\n","\n","  # Compute the qr factorization\n","  u, s, v = torch.svd(flattened, some=True)\n","  if rows < cols:\n","    u.t_()\n","  q = u if tuple(u.shape) == (rows, cols) else v\n","  with torch.no_grad():\n","    tensor.view_as(q).copy_(q)\n","    tensor.mul_(gain)\n","  return tensor\n","\n","\n","class ActorCritic(nn.Module):\n","  def __init__(self, state_dim, action_dim, action_std_init):\n","    super(ActorCritic, self).__init__()\n","    self.action_dim = action_dim\n","    self.action_var = torch.full((action_dim,), action_std_init ** 2).to(device) \n","    # torch.full: https://pytorch.org/docs/stable/generated/torch.full.html\n","\n","    self.actor = nn.Sequential(\n","        nn.Linear(state_dim, 64),\n","        nn.Tanh(),\n","        nn.Linear(64, 64),\n","        nn.Tanh(),\n","        nn.Linear(64, action_dim),\n","        nn.Tanh()\n","    )\n","    self.critic = nn.Sequential(\n","        nn.Linear(state_dim, 64),\n","        nn.Tanh(),\n","        nn.Linear(64, 64),\n","        nn.Tanh(),\n","        nn.Linear(64, 1)\n","    )\n","    self.actor.apply(init_orthogonal_weights)\n","    self.critic.apply(init_orthogonal_weights)\n","  \n","  def set_action_std(self, new_action_std):\n","    self.action_var = torch.full((self.action_dim,), new_action_std ** 2).to(device)\n","\n","  def forward(self, state):\n","    action_mean = self.actor(state)\n","    cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)\n","    dist = MultivariateNormal(action_mean, cov_matrix)\n","\n","    action = dist.sample()\n","    log_prob = dist.log_prob(action)\n","\n","    return action.detach(), log_prob.detach()"],"metadata":{"id":"qpf__pTkYosm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PPOAgent:\n","  def __init__(self, state_dim, action_dim, action_std_init=0.6,\n","      actor_lr=5e-4, critic_lr=1e-3, batch_size=32, num_epochs=40, \n","      gamma=0.99, gae_lambda=0.95, policy_clip=0.2):\n","    \n","    self.action_std = action_std_init\n","    self.gamma = gamma\n","    self.gae_lambda = gae_lambda\n","    self.policy_clip = policy_clip\n","    self.num_epochs = num_epochs\n","\n","    self.memory = RolloutMemory(batch_size)\n","\n","    self.policy = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n","    self.optimizer = torch.optim.Adam([\n","        {'params': self.policy.actor.parameters(), 'lr': actor_lr},\n","        {'params': self.policy.critic.parameters(), 'lr': critic_lr}\n","    ])\n","    self.policy_old = ActorCritic(state_dim, action_dim, action_std_init).to(device)\n","    self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","    self.mse_loss = nn.MSELoss()\n","\n","  def set_action_std(self, new_action_std):\n","    self.action_std = new_action_std\n","    self.policy.set_action_std(new_action_std)\n","    self.policy_old.set_action_std(new_action_std)\n","  \n","  def decay_action_std(self, action_std_decay_rate, min_action_std):\n","    self.action_std -= action_std_decay_rate\n","    self.action_std = round(self.action_std, 4)\n","    if self.action_std <= min_action_std:\n","      self.action_std = min_action_std\n","      print(\"setting actor output action_std to min_action_std : \", self.action_std)\n","    else:\n","      print(\"setting actor output action_std to : \", self.action_std)\n","    self.set_action_std(self.action_std)\n","  \n","  def select_action(self, state):\n","    with torch.no_grad():\n","      state = torch.FloatTensor(state).to(device)\n","      action, log_prob = self.policy_old(state)\n","    \n","    self.memory.states.append(state)\n","    self.memory.actions.append(action)\n","    self.memory.log_probs.append(log_prob)\n","    \n","    return action.detach().cpu().numpy().flatten()\n","  \n","  def discounted_rewards(self):\n","    \"\"\"Monte Carlo estimate of returns\"\"\"\n","    rewards = []\n","    discounted_reward = 0\n","    for reward, done in zip(reversed(self.memory.rewards), reversed(self.memory.dones)):\n","      if done:\n","        discounted_reward = 0\n","      discounted_reward = reward + (self.gamma * discounted_reward)\n","      rewards.insert(0, discounted_reward)\n","    \n","    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n","    \n","    return rewards\n","  \n","  def evaluate(self, state, action):\n","    action_mean = self.policy.actor(state)\n","    action_var = self.policy.action_var.expand_as(action_mean) \n","    # torch.expand(): https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand\n","\n","    cov_matrix = torch.diag_embed(action_var).to(device)\n","    # torch.diag_embed(): https://pytorch.org/docs/stable/generated/torch.diag_embed.html\n","\n","    dist = MultivariateNormal(action_mean, cov_matrix)\n","\n","    if self.policy.action_dim == 1:\n","      action = action.reshape(-1, self.policy.action_dim)\n","\n","    log_probs = dist.log_prob(action)\n","    dist_entropy = dist.entropy()\n","    state_values = self.policy.critic(state)\n","\n","    return log_probs, state_values, dist_entropy\n","  \n","  def update(self):\n","    rewards = self.discounted_rewards()\n","    old_states = torch.squeeze(torch.stack(self.memory.states, dim=0)).detach().to(device)\n","    old_actions = torch.squeeze(torch.stack(self.memory.actions, dim=0)).detach().to(device)\n","    old_log_probs = torch.squeeze(torch.stack(self.memory.log_probs, dim=0)).detach().to(device)\n","\n","    for _ in range(self.num_epochs):\n","      log_probs, state_values, dist_entropy = self.evaluate(old_states, old_actions)\n","      state_values = torch.squeeze(state_values)\n","      ratio = torch.exp(log_probs - old_log_probs.detach())\n","\n","      advantages = rewards - state_values.detach()\n","      surr1 = ratio * advantages\n","      surr2 = torch.clamp(ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantages\n","\n","      loss = -torch.min(surr1, surr2) + 0.5 * self.mse_loss(state_values, rewards) - 0.01 * dist_entropy\n","\n","      self.optimizer.zero_grad()\n","      loss.mean().backward()\n","      self.optimizer.step()\n","\n","    self.policy_old.load_state_dict(self.policy.state_dict())\n","    self.memory.clear_memory()"],"metadata":{"id":"2G4ZKZwlYqMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import deque\n","from datetime import datetime\n","\n","def train(agent, num_episodes=2000, max_episode_length=400, update_every=2048):\n","  scores = []\n","  scores_window = deque(maxlen=100)\n","  time_step = 0\n","\n","  start_time = datetime.now().replace(microsecond=0)\n","\n","  for i_episode in range(1, num_episodes + 1):\n","    state = env.reset()\n","    score = 0\n","    for t in range(1, max_episode_length + 1):\n","      action = agent.select_action(state)\n","      state, reward, done, _ = env.step(action)\n","\n","      # state, action, log_prob are stored in agent.select_action()\n","      agent.memory.rewards.append(reward)\n","      agent.memory.dones.append(done)\n","\n","      score += reward\n","      time_step += 1\n","\n","      if time_step % update_every == 0:\n","        agent.update()\n","      \n","      if time_step % action_std_decay_freq == 0:\n","        agent.decay_action_std(action_std_decay_rate, min_action_std)\n","\n","      if done:\n","        break\n","      \n","    scores_window.append(score)\n","    scores.append(score)\n","    \n","    print('\\rEpisode {}\\tScore: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_window)), end='')\n","    if i_episode % 100 == 0:\n","      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n","\n","    if np.mean(scores_window) >= 200.0:\n","      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n","      break\n","\n","  end_time = datetime.now().replace(microsecond=0)\n","  print(\"Total training time  : \", end_time - start_time)\n","    \n","  return scores"],"metadata":{"id":"7id8yd9NYtAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('LunarLanderContinuous-v2')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","actor_lr = 1.5e-3\n","critic_lr = 2.5e-3\n","\n","batch_size = 256\n","num_epochs = 10\n","\n","action_std_init = 0.6\n","action_std_decay_rate = 0.05\n","min_action_std = 0.1\n","action_std_decay_freq = int(5e4) # timesteps\n","#action_std_decay_freq = 150 # episodes\n","\n","agent = PPOAgent(state_dim, action_dim, action_std_init, actor_lr, critic_lr, batch_size, num_epochs)\n","\n","ppo_scores = train(agent, num_episodes=3000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9Es9Lr9cCfP","outputId":"79d3c8a9-da6d-4c3a-eb5c-26ce7e195fcd","executionInfo":{"status":"ok","timestamp":1655778045759,"user_tz":-480,"elapsed":585335,"user":{"displayName":"Lab Tei","userId":"14661680927443254315"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100\tAverage Score: -314.58\n","Episode 200\tAverage Score: -174.40\n","Episode 300\tAverage Score: -65.13\n","Episode 400\tAverage Score: -30.08\n","setting actor output action_std to :  0.55\n","Episode 500\tAverage Score: 56.95\n","Episode 547\tScore: 78.82\tAverage Score: 67.06setting actor output action_std to :  0.5\n","Episode 600\tAverage Score: 91.82\n","Episode 675\tScore: 13.16\tAverage Score: 114.27setting actor output action_std to :  0.45\n","Episode 700\tAverage Score: 112.42\n","Episode 800\tAverage Score: 124.14\n","Episode 802\tScore: 123.16\tAverage Score: 124.67setting actor output action_std to :  0.4\n","Episode 900\tAverage Score: 140.52\n","Episode 936\tScore: 26.97\tAverage Score: 115.37setting actor output action_std to :  0.35\n","Episode 1000\tAverage Score: 106.22\n","Episode 1079\tScore: 6.13\tAverage Score: 183.61setting actor output action_std to :  0.3\n","Episode 1097\tScore: 187.29\tAverage Score: 200.03\n","Environment solved in 997 episodes!\tAverage Score 200.03\n","Total training time  :  0:09:44\n"]}]}]}