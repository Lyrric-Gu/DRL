{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG_LunarLander_Continuous.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMBpTq+SdgLfqU1DG1NXd7Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Colab Setting"],"metadata":{"id":"QbjBiK0UUwpL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnpnyQqhUuCi","executionInfo":{"status":"ok","timestamp":1655775196489,"user_tz":-480,"elapsed":31413,"user":{"displayName":"Lab Tei","userId":"14661680927443254315"}},"outputId":"6477e3f1-9f2a-42da-ea3c-a621c7f6b4f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,271 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n","Fetched 784 kB in 1s (927 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 155639 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.4.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.9)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.1)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting box2d-py\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"]}],"source":["# Rendering problems in Colab \n","# https://stackoverflow.com/questions/63250935/nameerror-name-base-is-not-defined-while-running-open-ai-gym-in-google-colab\n","!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","\n","!pip3 install box2d-py\n","!pip3 install gym[Box_2D]\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","#=========================================================#\n","\n","import gym\n","import math\n","import random\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","# why unwrapped\n","# https://stackoverflow.com/questions/53836136/why-unwrap-an-openai-gym\n","env = gym.make('CartPole-v0').unwrapped\n","\n","# set up matplotlib\n","is_ipython = 'inline' in matplotlib.get_backend()\n","if is_ipython:\n","    from IPython import display\n","\n","plt.ion()\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["## DDPG"],"metadata":{"id":"XR1k3c3HUzE5"}},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"F5ES0ZztU3jV"}},{"cell_type":"markdown","source":["The output of the actor network is an exact action, instead of a probability distribution over actions. That's deterministic policy."],"metadata":{"id":"pB48Ta9VmqxB"}},{"cell_type":"code","source":["class Actor(nn.Module):\n","  def __init__(self, state_dim, action_dim, max_action_value):\n","    super(Actor, self).__init__()\n","    self.fc1 = nn.Linear(state_dim, 64)\n","    self.fc2 = nn.Linear(64, 64)\n","    self.fc3 = nn.Linear(64, action_dim)\n","    self.max_action_value = max_action_value\n","  \n","  def forward(self, x):\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    return self.max_action_value * torch.tanh(self.fc3(x))\n","\n","class Critic(nn.Module):\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","    self.fc1 = nn.Linear(state_dim + action_dim, 64)\n","    self.fc2 = nn.Linear(64, 64)\n","    self.fc3 = nn.Linear(64, 1)\n","  \n","  def forward(self, state, action):\n","    input = torch.cat([state, action], 1)\n","    q = F.relu(self.fc1(input))\n","    q = F.relu(self.fc2(q))\n","    return self.fc3(q)"],"metadata":{"id":"iE7NzxDpVEFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The critic takes in state and action, outputs the q value."],"metadata":{"id":"jp5W7qG1nCr2"}},{"cell_type":"markdown","source":["### Replay Buffer"],"metadata":{"id":"jiZxmfcFVDeK"}},{"cell_type":"markdown","source":["Use just numpy array with some counters to implement Replay Buffer.\n","\n","I think it is more convenient than using namedtuple and deque."],"metadata":{"id":"_4BzYoAuWATu"}},{"cell_type":"code","source":["class ReplayBuffer(object):\n","  def __init__(self, state_dim, action_dim, capacity=int(1e6), batch_size=64):\n","    self.capacity = capacity\n","    self.batch_size = batch_size\n","    self.counter = 0\n","\n","    self.state = np.zeros((capacity, state_dim))\n","    self.action = np.zeros((capacity, action_dim))\n","    self.next_state = np.zeros((capacity, state_dim))\n","    self.reward = np.zeros((capacity, 1))\n","    self.done = np.zeros((capacity, 1))\n","\n","  def add(self, state, action, next_state, reward, done):\n","    index = self.counter % self.capacity\n","\n","    self.state[index] = state\n","    self.action[index] = action\n","    self.next_state[index] = next_state\n","    self.reward[index] = reward\n","    self.done[index] = done\n","\n","    self.counter += 1\n","  \n","  def sample(self, batch_size=64):\n","    # min handle the situation when buffer is not full\n","    indices_range = min(self.counter, self.capacity)\n","    indices = np.random.randint(0, indices_range, size=batch_size)\n","    return(\n","      torch.Tensor(self.state[indices]).to(device),\n","      torch.Tensor(self.action[indices]).to(device),\n","      torch.Tensor(self.next_state[indices]).to(device),\n","      torch.Tensor(self.reward[indices]).to(device),\n","      torch.Tensor(self.done[indices]).to(device)\n","    )\n","  \n","  def can_sample(self):\n","    return self.counter > self.batch_size"],"metadata":{"id":"6L12mf7MVYdD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ornstein-Uhlenbeck process"],"metadata":{"id":"7FyM6NMKWx8O"}},{"cell_type":"markdown","source":["A method to encourage exploration according to the DDPG paper since in DDPG there is no epsilon-greedy policy.\n","\n","This part is taken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py"],"metadata":{"id":"nHRypFfaW0mx"}},{"cell_type":"code","source":["class OUNoise(object):\n","  def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n","    self.mu           = mu\n","    self.theta        = theta\n","    self.sigma        = max_sigma\n","    self.max_sigma    = max_sigma\n","    self.min_sigma    = min_sigma\n","    self.decay_period = decay_period\n","    self.action_dim   = action_space.shape[0]\n","    self.low          = action_space.low\n","    self.high         = action_space.high\n","    self.reset()\n","      \n","  def reset(self):\n","    self.state = np.ones(self.action_dim) * self.mu\n","      \n","  def evolve_state(self):\n","    x  = self.state\n","    dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n","    self.state = x + dx\n","    return self.state\n","  \n","  def get_action(self, action, t=0): \n","    ou_state = self.evolve_state()\n","    self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n","    return np.clip(action + ou_state, self.low, self.high)"],"metadata":{"id":"BborFvnhXUEA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Agent"],"metadata":{"id":"RAC1_jhuXbeL"}},{"cell_type":"code","source":["from copy import deepcopy\n","\n","class DDPGAgent:\n","  def __init__(self, state_dim, action_dim, max_action_value, actor_learning_rate=1e-4, critic_learning_rate=1e-3, memory=int(1e6), tau=1e-3):\n","    self.tau = tau\n","    self.action_dim = action_dim\n","    self.max_action_value = max_action_value\n","\n","    self.actor = Actor(state_dim, action_dim, max_action_value).to(device)\n","    self.actor_target = deepcopy(self.actor)\n","    self.acotr_optimizer = optim.Adam(self.actor.parameters(), lr=actor_learning_rate)\n","\n","    self.critic = Critic(state_dim, action_dim)\n","    self.critic_target = deepcopy(self.critic)\n","    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_learning_rate)\n","\n","    self.replay_buffer = ReplayBuffer(state_dim, action_dim, capacity=memory)\n","\n","  # A way to increase exploration suggested by some blogs\n","  # the idea is add some Gaussian noise to the action\n","  # An alternative of OU-noise\n","  def process_action(self, action, exploration_noise=0.1):\n","    action += np.random.normal(0, self.max_action_value * exploration_noise, size=self.action_dim)\n","    action = action.clip(-self.max_action_value, self.max_action_value)\n","    return action\n","  \n","  def select_action(self, state):\n","    state = torch.FloatTensor(state).to(device)\n","    with torch.no_grad():\n","      action = self.actor(state).cpu().data.numpy() #.flatten\n","    \n","    action = self.process_action(action)\n","\n","    return action\n","\n","  def soft_update(self, online_model, target_model, tau=1e-3):\n","    for online_param, target_param in zip(online_model.parameters(), target_model.parameters()):\n","      target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)\n","\n","  def update(self, gamma=0.99):\n","    states, actions, next_states, rewards, dones = self.replay_buffer.sample() # Tensors\n","\n","    next_actions = self.actor_target(next_states).detach()\n","    next_q_values = self.critic_target(next_states, next_actions).detach()\n","    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n","    predict_q_values = self.critic(states, actions)\n","\n","    # Critic loss is the TD-error between the Q value of current and next timestep\n","    critic_loss = F.mse_loss(predict_q_values, target_q_values)\n","    self.critic_optimizer.zero_grad()\n","    critic_loss.backward()\n","    self.critic_optimizer.step()\n","\n","    # Actor loss is the mean of Q values given by the critic\n","    actor_loss = - self.critic(states, self.actor(states)).mean()\n","    self.acotr_optimizer.zero_grad()\n","    actor_loss.backward()\n","    self.acotr_optimizer.step()\n","\n","    self.soft_update(self.critic, self.critic_target, self.tau)\n","    self.soft_update(self.actor, self.actor_target, self.tau)\n","  \n","  def step(self, state, action, next_state, reward, done):\n","    self.replay_buffer.add(state, action, next_state, reward, done)\n","    if self.replay_buffer.can_sample():\n","      self.update()"],"metadata":{"id":"ORyQU6teXcjL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the final training, using OU-Noise can let the agent learn faster in the beginning, but it is always hard to converge. Using Gaussian Noise, on the other hand, will be slower for learning but the rate of convergence is higher."],"metadata":{"id":"VnlbhVUGSxM_"}},{"cell_type":"code","source":["from collections import deque\n","from datetime import datetime\n","\n","def train(agent, noise, num_episodes=2000, max_timestep=1000):\n","  scores = []\n","  scores_window = deque(maxlen=100)\n","  start_time = datetime.now().replace(microsecond=0)\n","  #timestep = 0   # used for OUNoise decaying\n","\n","  for i_episode in range(1, num_episodes+1):\n","    state = env.reset()\n","    #noise.reset()\n","    score = 0\n","    for t in range(max_timestep):\n","      action = agent.select_action(state)\n","      #action = noise.get_action(action, timestep)\n","\n","      next_state, reward, done, _ = env.step(action)\n","      agent.step(state, action, next_state, reward, done)\n","\n","      state = next_state\n","      score += reward\n","\n","      #timestep += 1\n","      \n","      if done:\n","        break\n","\n","    scores_window.append(score)\n","    scores.append(score)\n","\n","    print('\\rEpisode {}\\tScore: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_window)), end='')\n","    if i_episode % 100 == 0:\n","      print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n","    if np.mean(scores_window) >= 200.0:\n","      print('\\nEnvironment solved in {:d} episodes!\\tAverage Score {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n","      break\n","  \n","  end_time = datetime.now().replace(microsecond=0)\n","  print(\"Total training time  : \", end_time - start_time)\n","\n","  return scores"],"metadata":{"id":"VHviPb8rYdNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('LunarLanderContinuous-v2')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action_value = env.action_space.high[0]\n","\n","agent = DDPGAgent(state_dim, action_dim, max_action_value, actor_learning_rate=1e-4, critic_learning_rate=1e-3)\n","noise = OUNoise(env.action_space)\n","ddpg_scores = train(agent, noise, num_episodes=2000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iu1pI-1kWTKj","outputId":"f384ac61-0de3-4f1f-e0c9-56285f2f1820","executionInfo":{"status":"ok","timestamp":1655781619254,"user_tz":-480,"elapsed":4149327,"user":{"displayName":"Lab Tei","userId":"14661680927443254315"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100\tAverage Score: -583.05\n","Episode 200\tAverage Score: -273.12\n","Episode 300\tAverage Score: -159.59\n","Episode 400\tAverage Score: -87.22\n","Episode 500\tAverage Score: -80.85\n","Episode 600\tAverage Score: -60.64\n","Episode 700\tAverage Score: -49.90\n","Episode 800\tAverage Score: -49.01\n","Episode 900\tAverage Score: -15.95\n","Episode 1000\tAverage Score: 114.83\n","Episode 1100\tAverage Score: 176.67\n","Episode 1116\tScore: 253.45\tAverage Score: 201.06\n","Environment solved in 1016 episodes!\tAverage Score 201.06\n","Total training time  :  1:09:09\n"]}]},{"cell_type":"markdown","source":["## Conclusion"],"metadata":{"id":"Pwl21zharUDV"}},{"cell_type":"markdown","source":["My implementation of DDPG can solved LunarLanderContinuous environement within pretty decent timesteps. However, the actual consuming time is quite long. This may due to 4 networks are used. However, the trained policy is not always converged. This point out the weakness of DDPG: it is hard to extend to high dimensions problem.\n","\n","From my point of view, the poor convergence may caused by the exploration mechanism. In the DDPG paper, OU-Noise is used. Some will suggest using Gaussian Noise. But either of them can not outperform stochastic hehaviour policy regarding exploration.\n","\n","What's more, considering the final performance in continuous environment, using 4 networks is too expensive. The goal of using 4 networks is to maintain the stability of the underlying DQN algorithm. I personally feel that in DDPG, most of the efforts is spent on making DQN work in continuous environment, which to me like a detour.\n","\n","Policy Gradient is much straightforward in solving continuous environment. It inherently can output continous action, and has a good exploration ability without extra efforts of adding other functions. Also, the backpropagation in policy gradient makes more sense than DQN, and it will not encounters the problem of overoptimistic caused by max operation. The things we want to fix for policy gradient is that, we want to have a better way to update our networks, to make each update is guaranteed to improve theoretically. What's more, we want the algorithm to be more sample-efficent.\n","\n","All these problems is solved by PPO, which is the most popular algorithm nowadays."],"metadata":{"id":"OR1Ev2berVm1"}}]}